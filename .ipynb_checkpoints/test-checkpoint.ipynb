{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2.5, style = 'whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data.data_sampler import sample_adult_data\n",
    "from scripts.data.data_sampler import sample_census_data\n",
    "from scripts.data.data_sampler import sample_mushroom_data\n",
    "from scripts.core.contextual_bandit import run_contextual_bandit, run_contextual_bandit_single\n",
    "from scripts.algorithms.linear_thompson_sampling import LinTS\n",
    "from scripts.algorithms.posterior_bnn_sampling import PosteriorBNNSampling\n",
    "from scripts.algorithms.linucb import LinUcb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_route = os.getcwd()\n",
    "data_route = 'datasets'\n",
    "output = os.path.join(base_route,'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(data_type, num_contexts=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_type: Dataset from which to sample.\n",
    "        num_contexts: Number of contexts to sample.\n",
    "    Returns:\n",
    "        dataset: Sampled matrix with rows: (context, reward_1, ..., reward_num_act).\n",
    "        opt_rewards: Vector of expected optimal reward for each context.\n",
    "        opt_actions: Vector of optimal action for each context.\n",
    "        num_actions: Number of available actions.\n",
    "        context_dim: Dimension of each context.\n",
    "    \"\"\"\n",
    "    \n",
    "    if data_type == 'mushroom':\n",
    "        # Create mushroom dataset\n",
    "        num_actions = 2\n",
    "        context_dim = 117\n",
    "        file_name = os.path.join(base_route, data_route, 'mushrooms.csv')\n",
    "        dataset, opt_mushroom = sample_mushroom_data(file_name, num_contexts)\n",
    "        opt_rewards, opt_actions = opt_mushroom\n",
    "    elif data_type == 'census':\n",
    "        file_name = os.path.join(base_route, data_route, 'USCensus1990.data.txt')\n",
    "        num_actions = 9\n",
    "        num_contexts = min(150000, num_contexts)\n",
    "        sampled_vals = sample_census_data(file_name, num_contexts,\n",
    "                                          shuffle_rows=True)\n",
    "        contexts, rewards, (opt_rewards, opt_actions) = sampled_vals\n",
    "        dataset = np.hstack((contexts, rewards))\n",
    "        context_dim = contexts.shape[1]\n",
    "    else:\n",
    "        file_name = os.path.join(base_route, data_route, 'adult.data')\n",
    "        num_actions = 14\n",
    "        num_contexts = min(45222, num_contexts)\n",
    "        sampled_vals = sample_adult_data(file_name, num_contexts,\n",
    "                                         shuffle_rows=True)\n",
    "        contexts, rewards, (opt_rewards, opt_actions) = sampled_vals\n",
    "        dataset = np.hstack((contexts, rewards))\n",
    "        context_dim = contexts.shape[1]\n",
    "        \n",
    "    return dataset, opt_rewards, opt_actions, num_actions, context_dim\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_contexts = 2000\n",
    "\n",
    "# Data type in {linear, sparse_linear, mushroom, financial, jester,\n",
    "#                 statlog, adult, covertype, census, wheel}\n",
    "data_type = 'mushroom'\n",
    "\n",
    "# Create dataset\n",
    "sampled_vals = sample_data(data_type, num_contexts)\n",
    "dataset, opt_rewards, opt_actions, num_actions, context_dim = sampled_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_ucb = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                               context_dim=context_dim,\n",
    "                                               alpha=10.0)\n",
    "hparams_linear = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                               context_dim=context_dim,\n",
    "                                               a0=6,\n",
    "                                               b0=6,\n",
    "                                               lambda_prior=0.25,\n",
    "                                               initial_pulls=2)\n",
    "hparams_bbb = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                            context_dim=context_dim,\n",
    "                                            init_scale=0.3,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            layer_sizes=[50],\n",
    "                                            batch_size=512,\n",
    "                                            activate_decay=True,\n",
    "                                            initial_lr=0.1,\n",
    "                                            max_grad_norm=5.0,\n",
    "                                            show_training=False,\n",
    "                                            freq_summary=1000,\n",
    "                                            buffer_s=-1,\n",
    "                                            initial_pulls=2,\n",
    "                                            optimizer='RMS',\n",
    "                                            use_sigma_exp_transform=True,\n",
    "                                            cleared_times_trained=10,\n",
    "                                            initial_training_steps=100,\n",
    "                                            noise_sigma=0.1,\n",
    "                                            reset_lr=False,\n",
    "                                            training_freq=50,\n",
    "                                            training_epochs=100)\n",
    "hparams_gp = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                           num_outputs=num_actions,\n",
    "                                           context_dim=context_dim,\n",
    "                                           reset_lr=False,\n",
    "                                           learn_embeddings=True,\n",
    "                                           max_num_points=1000,\n",
    "                                           show_training=False,\n",
    "                                           freq_summary=1000,\n",
    "                                           batch_size=512,\n",
    "                                           keep_fixed_after_max_obs=True,\n",
    "                                           training_freq=50,\n",
    "                                           initial_pulls=2,\n",
    "                                           training_epochs=100,\n",
    "                                           lr=0.01,\n",
    "                                           buffer_s=-1,\n",
    "                                           initial_lr=0.001,\n",
    "                                           lr_decay_rate=0.0,\n",
    "                                           optimizer='RMS',\n",
    "                                           task_latent_dim=5,\n",
    "                                           activate_decay=False)\n",
    "hparams_rms = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                            context_dim=context_dim,\n",
    "                                            init_scale=0.3,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            layer_sizes=[50],\n",
    "                                            batch_size=512,\n",
    "                                            activate_decay=True,\n",
    "                                            initial_lr=0.1,\n",
    "                                            max_grad_norm=5.0,\n",
    "                                            show_training=False,\n",
    "                                            freq_summary=1000,\n",
    "                                            buffer_s=-1,\n",
    "                                            initial_pulls=2,\n",
    "                                            optimizer='RMS',\n",
    "                                            reset_lr=True,\n",
    "                                            lr_decay_rate=0.5,\n",
    "                                            training_freq=50,\n",
    "                                            training_epochs=100,\n",
    "                                            p=0.95,\n",
    "                                            q=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [\n",
    "      LinUcb('LinUCB', hparams_ucb, output),\n",
    "      PosteriorBNNSampling('BBB', hparams_bbb, output, 'Variational'),\n",
    "      LinTS('LinFullPost', hparams_linear),\n",
    "      BootstrappedBNNSampling('BootRMS', hparams_rms, output),\n",
    "      PosteriorBNNSampling('MultitaskGP', hparams_gp, output, 'GP'),      \n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_contextual_bandit(context_dim, num_actions, dataset, algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " _, h_rewards = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cumulative Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "for i, a in enumerate(algos):\n",
    "    ax.plot(np.arange(0,2000),np.cumsum(h_rewards[:,i]),label= a.name)\n",
    "plt.legend(loc= 'best')\n",
    "plt.xlabel('rounds')\n",
    "plt.ylabel('cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cumulative Regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "for i, a in enumerate(algos):\n",
    "    ax.plot(np.arange(0,2000),np.cumsum(opt_rewards - h_rewards[:,i]),label= a.name)\n",
    "plt.legend(loc= 'best')\n",
    "plt.xlabel('rounds')\n",
    "plt.ylabel('cumulative Regret')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
